[ { "title": "Ghidra Scripting - XRefs and Decompilation", "url": "/ghidra-scripting-xrefs/", "categories": "Linux", "tags": "Reverse Engineering", "date": "2023-11-14 00:00:00 +0100", "snippet": "Hello everyone,I’m excited to share the recent experiments I’ve been conducting with Ghidra scripting. The capabilities of Ghidra’s decompiler and scripting tools truly stand out, surpassing what o...", "content": "Hello everyone,I’m excited to share the recent experiments I’ve been conducting with Ghidra scripting. The capabilities of Ghidra’s decompiler and scripting tools truly stand out, surpassing what other disassemblers offer. Ghidra’s headless mode particularly empowers researchers, granting the ability to extract essential information from binaries via the command line. The process of individually adding each binary to an active Ghidra project and running separate analyses can be tedious and time-consuming.There have been numerous instances where I’ve needed to swiftly extract firmware and analyze multiple binaries to uncover cross-references related to specific C library function calls. This, in my view, is an area where Ghidra scripting excels. To initiate this process, the first step involves the installation of Ghidra 10.3.3 and running the software.sudo apt install openjdk-17-jdkwget https://github.com/NationalSecurityAgency/ghidra/releases/download/Ghidra_10.3.3_build/ghidra_10.3.3_PUBLIC_20230829.zipunzip ghidra_10.3.3_PUBLIC_20230829.zipghidra_10.3.3_PUBLIC/./ghidraRunFollowing this, the recommended next step is to establish a ‘ghidra_scripts’ folder within your user’s home directory or any preferred location. Subsequently, I suggest creating a Ghidra project dedicated to a specific binary. Once done, add your directory by navigating to Window -&gt; Bundle Manager.Once our Ghidra scripts path is integrated into the Bundle Manager, the next step involves closing the current project and delving into code creation. A valuable example showcasing the utilization of Ghidra’s headless mode for automated binary analysis can be found in this GitHub repository: https://github.com/h4sh5/ghidra-headless-decompile/tree/master. To enable the simultaneous analysis of multiple binaries, I established a dedicated ‘binaries’ folder, housing each separate binary for analysis. I maintain the Ghidra projects post-analysis, ensuring their availability for further examination if needed. Enclosed is the source code for my initial ‘headless_analyzer.py’ script, designed to execute Ghidra’s ‘analyzeHeadless’ shell script for each binary and subsequently run the ‘analyzer.py’ script to dump decompilation results.#!/usr/bin/env python3import osimport subprocessimport timeimport reGHIDRA_PATH = os.path.expanduser(\"~/ghidra_10.3.3_PUBLIC\")GHIDRA_SCRIPT_PATH = os.path.expanduser(\"~/ghidra_scripts\")CURRENT_DIR = os.getcwd()print(\"---------------------Started Analyzing------------------------\")print(\"\")start_time = time.time()binaries_path = os.path.join(CURRENT_DIR, \"binaries\")root_results_directory = os.path.join(CURRENT_DIR, \"root_results\")os.makedirs(root_results_directory, exist_ok=True)for fileName in os.listdir(binaries_path): binary_path = os.path.join(binaries_path, fileName) results_directory = os.path.join(root_results_directory, f\"{fileName}_results\") os.makedirs(results_directory, exist_ok=True) exported_source = os.path.join(results_directory, f\"{fileName}_exported_source.c\") # Run Ghidra Headless ghidra_project_name = f\"{fileName}_ghidra_project\" subprocess.run([ f\"{GHIDRA_PATH}/support/analyzeHeadless\", results_directory, ghidra_project_name, \"-import\", binary_path, \"-scriptPath\", GHIDRA_SCRIPT_PATH, \"-postscript\", \"analyzer.py\", exported_source ])end_time = time.time()elapsed_time = round(end_time - start_time)print(\"\")print(\"---------------------Finished Analyzing------------------------\")print(f\"Elapsed time: {elapsed_time} seconds\")Subsequently, I’ve developed the preliminary code for my ‘analyzer.py’ script, leveraging Ghidra’s embedded CppExporter functionality to retrieve the decompilation results.#!/usr/bin/env python2#@author ReconDeveloper#@category #@keybinding #@menupath #@toolbar from ghidra.app.decompiler import DecompInterface, DecompileOptionsfrom ghidra.framework.plugintool.util import OptionsServicefrom ghidra.program.model.listing import CodeUnitfrom ghidra.program.model.symbol import *from ghidra.program.model.listing import * from ghidra.program.model.address import *from ghidra.app.util import Optionfrom ghidra.util.task import TaskMonitorfrom java.io import Filefrom ghidra.app.util.exporter import CppExporterfrom re import search# `currentProgram` or `getScriptArgs` function is contained in `__main__`import __main__ as ghidra_appdef run(): # getScriptArgs gets argument for this python script using `analyzeHeadless` args = ghidra_app.getScriptArgs() exporter = CppExporter() options = [Option(CppExporter.CREATE_HEADER_FILE, False)] exporter.setOptions(options) exporter.setExporterServiceProvider(state.getTool()) print(args[0]) f = File(args[0]) exporter.export(f, ghidra_app.currentProgram, None, TaskMonitor.DUMMY)if __name__ == '__main__': run()The output from executing the ‘headless_analyzer.py’ script is now visible, revealing the decompiled results for all three binaries alongside their corresponding Ghidra projects. While the exported source is comprehensive, encompassing not just ‘.text’ code but also ELF header information and thunk functions, there’s room for improvement. To refine the results, excluding dead code, thunk functions, and external symbols from the analysis could enhance the quality of the output. Below, I’ve included updated code for ‘headless_analyzer.py’ to integrate a ‘cleaned source’ functionality.#!/usr/bin/env python3import osimport subprocessimport timeimport reGHIDRA_PATH = os.path.expanduser(\"~/ghidra_10.3.3_PUBLIC\")GHIDRA_SCRIPT_PATH = os.path.expanduser(\"~/ghidra_scripts\")CURRENT_DIR = os.getcwd()print(\"---------------------Started Analyzing------------------------\")print(\"\")start_time = time.time()binaries_path = os.path.join(CURRENT_DIR, \"binaries\")root_results_directory = os.path.join(CURRENT_DIR, \"root_results\")os.makedirs(root_results_directory, exist_ok=True)for fileName in os.listdir(binaries_path): binary_path = os.path.join(binaries_path, fileName) results_directory = os.path.join(root_results_directory, f\"{fileName}_results\") os.makedirs(results_directory, exist_ok=True) cleaned_source = os.path.join(results_directory, f\"{fileName}_cleaned_source.c\") exported_source = os.path.join(results_directory, f\"{fileName}_exported_source.c\") # Run Ghidra Headless ghidra_project_name = f\"{fileName}_ghidra_project\" subprocess.run([ f\"{GHIDRA_PATH}/support/analyzeHeadless\", results_directory, ghidra_project_name, \"-import\", binary_path, \"-scriptPath\", GHIDRA_SCRIPT_PATH, \"-postscript\", \"analyzer.py\", cleaned_source, exported_source ])end_time = time.time()elapsed_time = round(end_time - start_time)print(\"\")print(\"---------------------Finished Analyzing------------------------\")print(f\"Elapsed time: {elapsed_time} seconds\")I’ve successfully implemented the functionality to print decompilation while excluding external symbols, dead code, and thunk functions. This enhancement took considerable effort, involving extensive research across multiple articles with similar objectives and cross-referencing against the Ghidra API.#!/usr/bin/env python2#@author ReconDeveloper#@category #@keybinding #@menupath #@toolbar from ghidra.app.decompiler import DecompInterface, DecompileOptionsfrom ghidra.framework.plugintool.util import OptionsServicefrom ghidra.program.model.listing import CodeUnitfrom ghidra.program.model.symbol import *from ghidra.program.model.listing import * from ghidra.program.model.address import *from ghidra.app.util import Optionfrom ghidra.util.task import TaskMonitorfrom java.io import Filefrom ghidra.app.util.exporter import CppExporter# `currentProgram` or `getScriptArgs` function is contained in `__main__`import __main__ as ghidra_appclass Analyzer: def __init__(self, program=None, timeout=None): # Initialize decompiler with current program self._decompiler = DecompInterface() self._decompiler.openProgram(program or ghidra_app.currentProgram) self._options = DecompileOptions() self._tool = state.getTool() self._timeout = timeout def set_up_decompiler(self): if self._tool is not None: options_service = self._tool.getService(OptionsService) if options_service is not None: tool_options = options_service.getOptions(\"Decompiler\") self._options.grabFromToolAndProgram(None, tool_options, program) #eliminate dead code self._options.setEliminateUnreachable(True) self._decompiler.setOptions(self._options) self._decompiler.toggleCCode(True) self._decompiler.toggleSyntaxTree(True) self._decompiler.setSimplificationStyle(\"decompile\") return self._decompiler def get_all_functions(self): st = ghidra_app.currentProgram.getSymbolTable() si = st.getSymbolIterator() symbol_dict = {} funcs = [] while si.hasNext(): s = si.next() if ((s.getSymbolType() == SymbolType.FUNCTION) and (not s.isExternal()) and (not s.getName() in symbol_dict.keys())): symbol_dict[s.getName()] = s.getAddress() for address in symbol_dict.values(): funcs.append(getFunctionAt(address)) return funcs def decompile_func(self, func): # Decompile self._decompiler = self.set_up_decompiler() decomp_results = self._decompiler.decompileFunction(func, 0, self._timeout) if (decomp_results is not None) and (decomp_results.decompileCompleted()): return decomp_results.getDecompiledFunction().getC() return \"\" def decompile(self): pseudo_c = '' # Enumerate all functions and decompile each function funcs = self.get_all_functions() for func in funcs: if not func.isThunk(): dec_func = self.decompile_func(func) if dec_func: pseudo_c += dec_func return pseudo_c def run(): # getScriptArgs gets argument for this python script using `analyzeHeadless` args = ghidra_app.getScriptArgs() analyzer = Analyzer() decompiled_source_file = args[0] # Do decompilation process pseudo_c = analyzer.decompile() # Save to output file with open(decompiled_source_file, 'w') as fw: fw.write(pseudo_c) print('[*] saving decompilation to -&gt; {}'.format(decompiled_source_file)) exporter = CppExporter() options = [Option(CppExporter.CREATE_HEADER_FILE, False)] exporter.setOptions(options) exporter.setExporterServiceProvider(analyzer._tool) f = File(args[1]) exporter.export(f, ghidra_app.currentProgram, None, TaskMonitor.DUMMY)if __name__ == '__main__': run()Upon executing the ‘headless_analyzer.py’ script, we now have refined source files for all of the binaries. This marks the culmination of the article and fulfills the main objective outlined at the beginning - obtaining cross-references to specific C library function calls. Below, I’ve included the final versions of the ‘headless_analyzer.py’ and ‘analyzer.py’ scripts essential for achieving this specific task.#!/usr/bin/env python3import osimport subprocessimport timeimport reGHIDRA_PATH = os.path.expanduser(\"~/ghidra_10.3.3_PUBLIC\")GHIDRA_SCRIPT_PATH = os.path.expanduser(\"~/ghidra_scripts\")CURRENT_DIR = os.getcwd()print(\"---------------------Started Analyzing------------------------\")print(\"\")start_time = time.time()binaries_path = os.path.join(CURRENT_DIR, \"binaries\")root_results_directory = os.path.join(CURRENT_DIR, \"root_results\")os.makedirs(root_results_directory, exist_ok=True)for fileName in os.listdir(binaries_path): binary_path = os.path.join(binaries_path, fileName) results_directory = os.path.join(root_results_directory, f\"{fileName}_results\") os.makedirs(results_directory, exist_ok=True) result_xrefs = os.path.join(results_directory, f\"{fileName}_xrefs.txt\") cleaned_source = os.path.join(results_directory, f\"{fileName}_cleaned_source.c\") exported_source = os.path.join(results_directory, f\"{fileName}_exported_source.c\") # Run Ghidra Headless ghidra_project_name = f\"{fileName}_ghidra_project\" subprocess.run([ f\"{GHIDRA_PATH}/support/analyzeHeadless\", results_directory, ghidra_project_name, \"-import\", binary_path, \"-scriptPath\", GHIDRA_SCRIPT_PATH, \"-postscript\", \"analyzer.py\", result_xrefs, cleaned_source, exported_source ])end_time = time.time()elapsed_time = round(end_time - start_time)print(\"\")print(\"---------------------Finished Analyzing------------------------\")print(f\"Elapsed time: {elapsed_time} seconds\")#!/usr/bin/env python2#@author ReconDeveloper#@category #@keybinding #@menupath #@toolbar from ghidra.app.decompiler import DecompInterface, DecompileOptionsfrom ghidra.framework.plugintool.util import OptionsServicefrom ghidra.program.model.listing import CodeUnitfrom ghidra.program.model.symbol import *from ghidra.program.model.listing import * from ghidra.program.model.address import *from ghidra.app.util import Optionfrom ghidra.util.task import TaskMonitorfrom java.io import Filefrom ghidra.app.util.exporter import CppExporterfrom re import search# `currentProgram` or `getScriptArgs` function is contained in `__main__`import __main__ as ghidra_appclass Analyzer: def __init__(self, program=None, timeout=None): # Initialize decompiler with current program self._decompiler = DecompInterface() self._decompiler.openProgram(program or ghidra_app.currentProgram) self._options = DecompileOptions() self._tool = state.getTool() self._timeout = timeout def set_up_decompiler(self): if self._tool is not None: options_service = self._tool.getService(OptionsService) if options_service is not None: tool_options = options_service.getOptions(\"Decompiler\") self._options.grabFromToolAndProgram(None, tool_options, program) # eliminate dead code self._options.setEliminateUnreachable(True) self._decompiler.setOptions(self._options) self._decompiler.toggleCCode(True) self._decompiler.toggleSyntaxTree(True) self._decompiler.setSimplificationStyle(\"decompile\") return self._decompiler def get_all_functions(self): st = ghidra_app.currentProgram.getSymbolTable() si = st.getSymbolIterator() symbol_dict = {} funcs = [] while si.hasNext(): s = si.next() if ((s.getSymbolType() == SymbolType.FUNCTION) and (not s.isExternal()) and (not s.getName() in symbol_dict.keys())): symbol_dict[s.getName()] = s.getAddress() for address in symbol_dict.values(): funcs.append(getFunctionAt(address)) return funcs def decompile_func(self, func): # Decompile self._decompiler = self.set_up_decompiler() decomp_results = self._decompiler.decompileFunction(func, 0, self._timeout) if (decomp_results is not None) and (decomp_results.decompileCompleted()): return decomp_results.getDecompiledFunction().getC() return \"\" def decompile(self): pseudo_c = '' # Enumerate all functions and decompile each function funcs = self.get_all_functions() for func in funcs: if not func.isThunk(): dec_func = self.decompile_func(func) if dec_func: pseudo_c += dec_func return pseudo_c def list_cross_references(self, dst_func, tag, output_path): dst_name = dst_func.getName() dst_addr = dst_func.getEntryPoint() references = getReferencesTo(dst_addr) # limited to 4096 records xref_addresses = [] f = open(output_path,'a') for xref in references: if xref.getReferenceType().isCall(): call_addr = xref.getFromAddress() src_func = getFunctionContaining(call_addr) if src_func is not None: xref_addresses.append(src_func.getEntryPoint()) if ((not src_func.isThunk()) and (xref_addresses.count(src_func.getEntryPoint()) &lt; 2)): results = str(self.decompile_func(src_func)) for line in results.splitlines(): if search(dst_name, line): print &gt;&gt;f, \"Call to {} in {} at {} has function signature of: {}\" \\ .format(dst_name,src_func.getName(), \\ call_addr, line) f.close() def get_imported_functions(self, output_path): import_functions = [ # No bounds checking, buffer overflows common \"strcpy\", \"sprintf\", \"vsprintf\", \"strcat\", \"getpass\", \"strlen\", #needs null terminator! # Windows specific functions, buffer overflows common \"makepath\", \"_makepath\", \"_splitpath\", \"snscanf\", \"_snscanf\", # Copy functions Windows API and kernel driver functions \"RtlCopyMemory\", \"CopyMemory\", # When given %s specifier, can cause overflow, if scanf(\"%10s\", buf) still check size of buffer to see if smaller \"scanf\", \"fscanf\", \"sscanf\", \"__isoc99_scanf\", \"__isoc99_fscanf\", \"__isoc99_sscanf\", # Often bounds is based on size of input \"snprintf\", \"strncpy\", \"strncat\", # Printf functions, check for format string \"printf\", \"fprintf\", # Check for insecure use of environment variables \"getenv\", # Check if size arg can contain negative numbers or zero, return value must be checked \"malloc\", # Potential implicit overflow due to integer wrapping \"calloc\", # Doesn't initialize memory to zero; realloc(0) is equivalent to free \"realloc\", # check for incorrect use, double free, use after free \"free\", \"_free\", # I/O functions \"fgets\", \"fread\", \"fwrite\", \"read\", \"recv\", \"recvfrom\", \"write\", # Check for command injection and shell exploitation (runs with shell on machine) \"system\", \"popen\", # Check for command injection and # File descriptor handling, might inherit open file descriptors from calling process # If sensitive file descriptors are left open or not handled correctly, it can lead to information leak \"execl\", \"execlp\", \"execle\", \"execv\", \"execve\", \"execvp\", \"execvpe\", # Common static memory copy functions in libc \"memcpy\", \"memset\", \"bcopy\"] tag = \"Imported Function\" st = ghidra_app.currentProgram.getSymbolTable() si = st.getSymbolIterator() symbol_dict = {} funcs = [] while si.hasNext(): s = si.next() if ((s.getSymbolType() == SymbolType.FUNCTION) and (not s.isExternal()) and (s.getName() in import_functions) and (not s.getName() in symbol_dict.keys())): symbol_dict[s.getName()] = s.getAddress() for address in symbol_dict.values(): funcs.append(getFunctionAt(address)) for f in funcs: self.list_cross_references(f,tag,output_path) def run(): # getScriptArgs gets argument for this python script using `analyzeHeadless` args = ghidra_app.getScriptArgs() f = open(args[0],'w') print &gt;&gt;f, 'Xref Results \\n-----------------------------\\n' f.close() analyzer = Analyzer() analyzer.get_imported_functions(args[0]) decompiled_source_file = args[1] # Perform selective decompilation process pseudo_c = analyzer.decompile() # Save to output file with open(decompiled_source_file, 'w') as fw: fw.write(pseudo_c) print('[*] saving decompilation to -&gt; {}'.format(decompiled_source_file)) exporter = CppExporter() options = [Option(CppExporter.CREATE_HEADER_FILE, False)] exporter.setOptions(options) exporter.setExporterServiceProvider(analyzer._tool) f = File(args[2]) exporter.export(f, ghidra_app.currentProgram, None, TaskMonitor.DUMMY)if __name__ == '__main__': run()The output from running ‘headless_analyzer.py’ now incorporates an ‘xrefs.txt’ file, cataloging the cross-references for all the analyzed binaries. This ability to obtain cross-references to C library function calls across multiple binaries through headless scripting is an incredibly powerful feature. That is all for this blog post, I appreciate everyone taking the time to read it!" }, { "title": "Angr Overflow Analysis", "url": "/angr-full-binary/", "categories": "Linux", "tags": "Vulnerability Research, Symbolic Execution, Reverse Engineering", "date": "2023-11-12 00:00:00 +0100", "snippet": "Hello everyone,I wanted to share my recent experience using Angr to detect overflows in binaries. I’ve dedicated several late nights to crafting an Angr script capable of identifying these overflow...", "content": "Hello everyone,I wanted to share my recent experience using Angr to detect overflows in binaries. I’ve dedicated several late nights to crafting an Angr script capable of identifying these overflows and now I’m eager to share my findings. My quest has been to explore alternative methods for analyzing binaries for memory corruption. From my experience, automated static analyzers have proven unreliable in detecting memory corruption.Even when armed with the source code, I strongly advocate for the utilization of coverage-guided fuzzing tools such as AFL++ or libFuzzer to unearth vulnerabilities, favoring them over static analyzers. My approach primarily involves employing AFL++ in qemu mode for testing binaries. However, complexities arise when handling binaries that rely on socket communication over networks. AFL++, primarily designed for file-based fuzzing, requires a workaround involving desock and binary patching to interact with binaries using sockets. A notable demonstration of this technique can be found in Attify’s insightful blog, showcasing the process at https://blog.attify.com/fuzzing-iot-binaries-with-afl-part-ii/.In certain intricate scenarios, maneuvering through the complexities of patching binaries can be a daunting task. In these instances, I believe Angr becomes a valuable ally for researchers. I intend to delve into a specific recurrent case, delving into the insights gained while utilizing Angr to detect overflows. For this exploration, I’ll be referencing code from the following GitHub repository: https://github.com/shenfeng/tiny-web-server/tree/master. This code harbors a known buffer overflow, manifesting at line 255 within the url_decode function. My focus, however, won’t revolve around the discovery and exploitation of this overflow, as these aspects have been extensively covered. Instead, I aim to scrutinize prevalent solutions to discern if they effectively detect this overflow.The initial method I’ll employ is detailed in this article: https://security.humanativaspa.it/automating-binary-vulnerability-discovery-with-ghidra-and-semgrep/. The approach outlined in this article involves crafting semgrep rules and analyzing Ghidra’s decompilation against those rules. However, this method is contingent on the presence of the memcpy function call in the decompilation. Notably, it’s a common optimization by compilers to eliminate certain C library calls, including memcpy. In the context of the url_decode function, the memcpy call is optimized out in both IDA and Ghidra’s decompilation. Consequently, the absence of this call causes the semgrep rules to overlook the vulnerable section of code.\\Another technique worth discussing involves utilizing cwe_checker (https://github.com/fkie-cad/cwe_checker) to identify the vulnerability. However, upon executing cwe_checker, it does not flag any vulnerability within the url_decode function.Now, I’ll delve into my use of Angr to uncover the overflow within the url_decode function. Despite its effectiveness, leveraging Angr for the analysis of extensive binaries has presented challenges, notably due to path explosion. Symbolic execution, a core aspect of Angr, systematically explores all potential execution paths within a program. However, in the case of large binaries characterized by numerous conditional branches, loops, and intricate control flow, the sheer volume of potential paths can multiply rapidly, leading to what’s known as path explosion. This phenomenon significantly slows down the analysis process and often results in freezing my virtual machine.I highly recommend conducting Angr analyses on a VM or host equipped with ample CPU and memory resources to support the demanding nature of this process. In light of path explosion, I’ve often found greater success by directing Angr to a specific function address, initiating the analysis from that point onward. A great demonstration of this approach, focusing on pinpointing buffer overflows, is detailed in this gitbook: https://breaking-bits.gitbook.io/breaking-bits/vulnerability-discovery/automated-exploit-development/buffer-overflows.I embarked on the challenge of analyzing the full binary to uncover the memcpy buffer overflow. Through numerous hours dedicated to debugging, making necessary modifications, and enduring VM freezes, I ultimately achieved this task. My initial approach involved extracting all functions from the binary’s .text section, constructing a dictionary that Angr could reference during analysis. I specifically aimed to exclude any C library calls and functions attributed to the GCC toolchain. My objective was to obtain the start addresses for each instance the function was called, subsequently utilizing these addresses in Angr for overflow analysis. To accomplish this, I opted to employ radare2 for disassembling the binary due to its command-line interface and user-friendly nature. Below, I’ve included the code used for this stage.import r2pipeimport argparsedef get_function_xrefs(binary_path): r2 = r2pipe.open(binary_path) # Analyze the binary r2.cmd('aaa') # Get a list of functions functions = r2.cmdj('aflj') function_xrefs = {} excluded_functions = [\"sym.deregister_tm_clones\", \"sym.register_tm_clones\"] for function in functions: function_name = function['name'] if function_name in excluded_functions: continue # Check if the function is in the .text section and not an import if not function_name.startswith('sym.imp.'): # Analyze cross-references to the function xrefs = r2.cmdj(f'axtj {function_name}') if xrefs: # Extract the starting address of the call instruction call_start_addresses = [xref['from'] for xref in xrefs] function_xrefs[function_name] = call_start_addresses return function_xrefsif __name__ == '__main__': parser = argparse.ArgumentParser(description='Print start addresses of calls to functions') parser.add_argument('binary_path', type=str, help='Path to the binary') args = parser.parse_args() function_xrefs = get_function_xrefs(args.binary_path) total_xrefs = sum(len(xrefs) for xrefs in function_xrefs.values()) print(f\"Total Cross-references: {total_xrefs}\") for function, xrefs in function_xrefs.items(): print(f\"Function Name: {function}\") for xref in xrefs: print(f\" Call Start Address: {hex(xref)}\")The script’s output highlights 22 cross-references, with just one pointing to url_decode. The subsequent phase involves integrating this script logic into my Angr analysis. Enclosed below is the fully implemented script designed to retrieve all call start addresses and conduct buffer overflow analysis on each. To manage the load and mitigate potential VM freezes, I constrained the concurrent processes to 8. The analysis time per process was also limited to 5 seconds, a deliberate measure to address path explosion challenges. Although extending the analysis time could enhance path exploration, in this context, a shallow path traversal sufficed to pinpoint the overflow.import angrimport claripyimport sysimport loggingimport osimport r2pipeimport argparseimport multiprocessingimport time# Disable integer to string conversion limitsys.set_int_max_str_digits(0)def check_buffer_overflow(simgr, sym, output_folder): for path in simgr.unconstrained: if path.satisfiable(extra_constraints=[path.regs.pc == 0x4343434343434343]): bb_addrs = path.history.bbl_addrs.hardcopy potential_overflow_addr = None # Find the first occurrence of a basic block address that's different from the current one for i in range(1, len(bb_addrs)): if bb_addrs[i] != bb_addrs[i - 1]: potential_overflow_addr = bb_addrs[i - 1] break if potential_overflow_addr is not None: path.add_constraints(path.regs.pc == 0x4343434343434343) if path.satisfiable(): stdin_payload = path.solver.eval(sym, cast_to=bytes) save_overflow_info(output_folder, potential_overflow_addr, stdin_payload) return simgrdef save_overflow_info(output_folder, address, payload): output_file = os.path.join(output_folder, f\"overflow_{hex(address)}.txt\") with open(output_file, 'w') as f: f.write(f\"Address: {hex(address)}\\n\") f.write(f\"Payload: {payload.hex() }\\n\")def get_function_xrefs(binary_path, excluded_functions): r2 = r2pipe.open(binary_path) # Analyze the binary r2.cmd('aaa') # Get a list of functions functions = r2.cmdj('aflj') function_xrefs = {} for function in functions: function_name = function['name'] # Check if the function is in the .text section and not an import if not function_name.startswith('sym.imp.') and function_name not in excluded_functions: # Analyze cross-references to the function xrefs = r2.cmdj(f'axtj {function_name}') if xrefs: # Extract the starting address of the call instruction call_start_addresses = [xref['from'] for xref in xrefs] call_start_addresses_hex = [hex(addr) for addr in call_start_addresses] function_xrefs[function_name] = call_start_addresses_hex return function_xrefsdef analyze_function(binary, call_start_address, payload_size, output_folder, result_queue, timeout): project = angr.Project(binary, load_options={'auto_load_libs': False}) # Set up the initial state with symbolic stdin random_payload = os.urandom(payload_size) sym = claripy.BVV(random_payload) initial_state = project.factory.entry_state(addr=call_start_address, stdin=sym) # Create a new SimulationManager for analysis simgr_function = project.factory.simulation_manager(initial_state, save_unconstrained=True) # Analyze the control flow graph of the binary cfg = project.analyses.CFGFast() start_time = time.time() simgr_function.run(until=lambda simgr: time.time() - start_time &gt;= timeout or (len(simgr_function.unconstrained) &gt; 0 and any([path.satisfiable(extra_constraints=[path.regs.pc == call_start_address]) for path in simgr_function.unconstrained]))) # Check for buffer overflow simgr_function = check_buffer_overflow(simgr_function, sym, output_folder) result_queue.put((call_start_address, simgr_function.unconstrained))def main(args): # Set up Angr project project = angr.Project(args.binary, load_options={'auto_load_libs': False}) excluded_functions = [\"sym.deregister_tm_clones\", \"sym.register_tm_clones\"] function_xrefs = get_function_xrefs(args.binary, excluded_functions) results = [] result_queue = multiprocessing.Queue timeout = 5 pool = multiprocessing.Pool(processes=8) # Iterate over the functions and xrefs and use the pool to parallelize the analysis for function, call_start_addresses in function_xrefs.items(): for addr in call_start_addresses: addr_int = int(addr, 16) pool.apply_async(analyze_function, (args.binary, addr_int, args.payload_size, args.output_folder, result_queue, timeout)) pool.close() pool.join() # Collect the results while not result_queue.empty(): addr, unconstrained = result_queue.get() results.append((addr, unconstrained)) for addr, unconstrained in results: for path in unconstrained: passif __name__ == '__main__': parser = argparse.ArgumentParser(description='Angr script') parser.add_argument('binary', type=str, help='Path to the binary to analyze') parser.add_argument('--payload-size', type=int, default=4096, help='Size of the payload for analysis') parser.add_argument('--output-folder', type=str, default='output', help='Folder to save output files') args = parser.parse_args() if not os.path.exists(args.output_folder): os.makedirs(args.output_folder) logging.getLogger(\"angr\").setLevel(logging.CRITICAL) main(args)Upon running the script, Angr successfully identified multiple potential overflows. Specifically, Angr pinpointed a potential overflow within the url_decode function, displaying the payload used to trigger it. To execute this overflow, the provided code allows for the payload to be sent as an HTTP GET request, effectively triggering the identified overflow.import requestsimport reurl = \"http://127.0.0.1:9999\"with open('output/overflow_0x401e4a.txt', 'r') as file: payload = None for line in file: if line.startswith(\"Payload: \"): payload = line[len(\"Payload: \"):].strip() break# Send the payload as a GET requestresponse = requests.get(f\"{url}/{payload}\")if response.status_code == 200: print(\"Request successful\")else: print(f\"Request failed with status code: {response.status_code}\")Reflecting on the test, it’s evident that Angr showcases significant prowess in analyzing entire binaries to unveil shallow bugs. Moreover, when tasked with specific functions, Angr demonstrates exceptional capabilities. While the payload generation may not be immediately relevant, the ability to identify and isolate sections of code proves invaluable, enabling researchers to efficiently prioritize their reverse engineering efforts." } ]
